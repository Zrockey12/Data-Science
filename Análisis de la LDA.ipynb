{"cells":[{"cell_type":"markdown","metadata":{"id":"v_XmdRf_Qe3d"},"source":["# Análisis de la LDA (Latent Dirichlet Allocation)"]},{"cell_type":"markdown","metadata":{"id":"pszTrc8RQe3d"},"source":["Librerías a importar:"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2020-09-23T09:17:00.154062Z","start_time":"2020-09-23T09:16:57.515400Z"},"id":"17gTCD0xQe3e","init_cell":true},"outputs":[],"source":["import sys, re, time, string, random, csv, argparse\n","import requests\n","import numpy as n\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","from scipy.special import psi\n","\n","#Librerías de web scraping\n","from bs4 import BeautifulSoup\n","\n","from tqdm.notebook import tqdm\n","\n","#Librerías de NLP\n","import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","from nltk.tokenize import wordpunct_tokenize\n","\n","\n","print('\\n¡Librerías importadas con éxito!')"]},{"cell_type":"markdown","metadata":{"id":"rtJVGQOBQe3h"},"source":["# Generación de la base de datos (*Web Scraping*)"]},{"cell_type":"markdown","metadata":{"id":"sxL1oYykQe3i"},"source":["Obtenemos la lista de profesores del departamento de EECS, y el lab al que pertenecen. Por lo que se hará uso de librerías de Web scraping como BeautifulSoup."]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2020-05-26T18:50:33.912560Z","start_time":"2020-05-26T18:50:33.275504Z"},"id":"mvW0yp-mQe3i"},"outputs":[],"source":["#Url de la lista de profesores de EECS\n","url_eecs_fac = 'https://www.eecs.mit.edu/people/faculty-advisors'\n","#Descargando el código fuente de dicha url\n","html_data = requests.get(url_eecs_fac).text\n","soup = BeautifulSoup(html_data, 'html.parser')\n","#Guardando el nombre y el laboratorio en dos listas\n","fac = [el.text for el in soup.find_all(class_ = 'field-content card-title')]\n","labs = []\n","for lab_raw in soup.find_all(class_ = 'views-field views-field-term-node-tid'):\n","    try:\n","        labs.append(lab_raw.find('a').text)\n","    except:\n","        labs.append('')\n","#Uniendo ambas listas y eliminando los profesores que no pertenencen a CSAIL, MTL, RLE o LIDS\n","fac_dept = list(zip(fac,labs))\n","fac_dept = [tup for tup in fac_dept if tup[1] in set(['CSAIL','MTL','RLE','LIDS'])]\n","\n","print('Número de profesores de EECS: {}'.format(len(fac_dept)))\n","print(fac_dept[:5],'...')"]},{"cell_type":"markdown","metadata":{"id":"WX5QWgixQe3o"},"source":["Definimos una función para obtener todos los artículos de _arXiv_ de un autor determinado."]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2020-05-26T18:50:34.431309Z","start_time":"2020-05-26T18:50:33.920015Z"},"id":"9H-UOKsMQe3o"},"outputs":[],"source":["def get_articles_for_author(author):\n","    base_url = 'https://arxiv.org/search/?query=%22{name}%22&searchtype=author&abstracts=show&order=-announced_date_first&size=200'\n","    author_query_url = base_url.format(name= author.replace(' ','+'))\n","    query_result = requests.get(author_query_url).text\n","    soup = BeautifulSoup(query_result, 'html.parser')\n","    articles = soup.find_all(class_ = 'arxiv-result')\n","\n","    ids = [el.find(class_ = 'list-title is-inline-block').find('a').text.strip('arXiv:') for el in articles]\n","    ids = [el.split('/')[1] if el.find('/')>=0 else el for el in ids]\n","    \n","    titles = [el.find(class_ = 'title is-5 mathjax').text.strip(' \\n') for el in articles]\n","    authors = [[author.text for author in el.find(class_ = 'authors').find_all('a')] for el in articles]\n","    abstracts = []\n","    for el in articles:   \n","        try:\n","            abstracts.append(el.find(class_ = 'abstract-full has-text-grey-dark mathjax').text[9:-16])\n","        except:\n","            abstracts.append(el.find(class_ = 'abstract-short has-text-grey-dark mathjax').text[9:-16])\n","    urls = [el.find(class_ = 'list-title is-inline-block').find('a')['href'] for el in articles]\n","    \n","    return ids, titles, authors, urls, abstracts\n","\n","author = fac_dept[23][0]\n","print('Ejemplo de un artículo de {}: \\n'.format(author))\n","ids, titles, authors, urls, abstracts = get_articles_for_author(author)\n","i = 0\n","print('arXiv ID: {} (url: {} )'.format(ids[i],urls[i]))\n","print('Título: {}'.format(titles[i]))\n","print('Autores: {}'.format(authors[i]))\n","print('---\\n{}\\n---'.format(abstracts[i]))"]},{"cell_type":"markdown","metadata":{"id":"PvhQtDjCQe3s"},"source":["Una vez tenemos dicha función definida y hemos comprobado su funcionamiento, iteramos la lista de profesores para obtener los artículos de cada profesor."]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2020-05-26T18:51:33.145868Z","start_time":"2020-05-26T18:50:34.433174Z"},"id":"t8WoqigZQe3s"},"outputs":[],"source":["ids = []\n","titles = []\n","authors = []\n","labs = []\n","EECS_facs = []\n","urls = []\n","abstracts = []\n","print('Descargando artículos de cada profesor:')\n","t0 = time.time()\n","for i,fac in enumerate(fac_dept):\n","    id_list, title_list, author_list, url_list, abstract_list  = get_articles_for_author(fac[0])\n","    ids += id_list\n","    titles += title_list\n","    authors += author_list\n","    urls += url_list\n","    abstracts += abstract_list\n","    labs += [fac[1]]*len(id_list)\n","    EECS_facs += [fac[0]]*len(id_list)\n","    if round(i/10) == i/10:\n","        print('{}/{} autores'.format(i,len(fac_dept)))\n","tf = time.time()\n","print('{} artículos descargados en {:.2f}s'.format(len(ids),tf-t0))"]},{"cell_type":"markdown","metadata":{"id":"bopZxfTLQe3u"},"source":["Guardamos toda la información en una tabla de pandas:"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2020-05-26T18:51:33.183243Z","start_time":"2020-05-26T18:51:33.149941Z"},"id":"hW2W9OAEQe3u"},"outputs":[],"source":["df = pd.DataFrame({'id':ids,'title':titles,'EECS_prof':EECS_facs,'lab':labs,'authors':authors,'url':urls,'abstract':abstracts})\n","df[['id','lab']] = df[['id','lab']].drop_duplicates()\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"xVSp-7U6Qe3x"},"source":["## Obtención del vocabulario "]},{"cell_type":"markdown","metadata":{"id":"Rvtv367GQe3x"},"source":["Se hace uso del archivo Origial de su repositorio - LDA SVI: "]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2020-05-26T18:51:33.389396Z","start_time":"2020-05-26T18:51:33.186212Z"},"id":"Cd7uZr8zQe3y"},"outputs":[],"source":["url = 'https://raw.githubusercontent.com/blei-lab/onlineldavb/master/dictnostops.txt'\n","vocab_list = pd.Series(requests.get(url).text.split('\\n')[:-1])\n","vocab = {}\n","for index, word in enumerate(vocab_list):\n","    vocab[word] = index\n","vocab_list"]},{"cell_type":"markdown","metadata":{"ExecuteTime":{"end_time":"2020-05-20T21:11:08.244070Z","start_time":"2020-05-20T21:11:08.241636Z"},"id":"I4sLQrz0Qe30"},"source":["# LDA-SVI"]},{"cell_type":"markdown","metadata":{"ExecuteTime":{"end_time":"2020-05-25T21:42:27.687982Z","start_time":"2020-05-25T21:42:27.685864Z"},"id":"AlpT9pV7Qe30"},"source":["## Funciones auxiliares"]},{"cell_type":"markdown","metadata":{"heading_collapsed":true,"id":"dqheWHoIQe33"},"source":["### Generación de atributos de cada texto (recuento de palabras)"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2020-05-26T18:51:33.480797Z","start_time":"2020-05-26T18:51:33.399279Z"},"hidden":true,"id":"qjN28qlFQe33"},"outputs":[],"source":["def parseDocument(doc, vocab):\n","    wordslist = list()\n","    countslist = list()\n","    doc = doc.lower()\n","    tokens = wordpunct_tokenize(doc)\n","    \n","    dictionary = dict()\n","    for word in tokens:\n","        if word in vocab:\n","            wordtk = vocab[word]\n","            if wordtk not in dictionary:\n","                dictionary[wordtk] = 1\n","            else:\n","                dictionary[wordtk] += 1\n","\n","    wordslist.append(list(dictionary.keys()))\n","    countslist.append(list(dictionary.values()))\n","    return (wordslist[0], countslist[0])"]},{"cell_type":"markdown","metadata":{"heading_collapsed":true,"id":"f5MuLE7qQe35"},"source":["### Cálculo de distribuciones de probabilidad"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2020-05-26T18:51:33.489385Z","start_time":"2020-05-26T18:51:33.482373Z"},"hidden":true,"id":"Sltn_TgzQe36"},"outputs":[],"source":["def dirichlet_expectation(alpha):\n","    '''\n","    For a vector theta ~ Dir(alpha), computes E[log(theta)] given alpha.\n","    \n","    Taken from https://github.com/blei-lab/onlineldavb/blob/master/onlineldavb.py\n","    '''\n","    if (len(alpha.shape) == 1):\n","        return (psi(alpha) - psi(n.sum(alpha)))\n","    return (psi(alpha) - psi(n.sum(alpha, 1))[:, n.newaxis])\n","\n","def beta_expectation(a, b, k):\n","    mysum = psi(a + b)\n","    Elog_a = psi(a) - mysum\n","    Elog_b = psi(b) - mysum\n","    Elog_beta = n.zeros(k)\n","    Elog_beta[0] = Elog_a[0]\n","    # print Elog_beta\n","    for i in range(1, k):\n","        Elog_beta[i] = Elog_a[i] + n.sum(Elog_b[0:i])\n","        # print Elog_beta\n","    # print Elog_beta\n","    return Elog_beta\n","\n","def plottrace(x, Y, K, n, perp):\n","    for i in range(K):\n","        plt.plot(x, Y[i], label = \"Topic %i\" %(i+1))\n","\n","    plt.xlabel(\"Number of Iterations\")\n","    plt.ylabel(\"Probability of Each topic\")\n","    plt.legend()\n","    plt.title(\"Trace plot for topic probabilities\")\n","    plt.savefig(\"temp/plot_%i_%i_%f.png\" %(K, n, perp))"]},{"cell_type":"markdown","metadata":{"id":"ygL8w3ePQe38"},"source":["Implementación de la LDA mediante SVI"]},{"cell_type":"markdown","metadata":{"id":"xQO6bGQTQe38"},"source":["Obtenemos la implementación de LDA mediante SVI de las fuentes mencionadas en el guión:\n","* https://github.com/qlai/stochasticLDA\n","* https://github.com/blei-lab/onlineldavb"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2020-05-26T18:51:33.528694Z","start_time":"2020-05-26T18:51:33.490967Z"},"id":"gjUOfgycQe38"},"outputs":[],"source":["n.random.seed(10000001)\n","meanchangethresh = 1e-3\n","MAXITER = 10000\n","\n","class SVILDA():\n","    \"\"\"\n","        Arguments:\n","        K: Number of topics\n","        vocab: A set of words to recognize. When analyzing documents, any word\n","           not in this set will be ignored.\n","        D: Total number of documents in the population. For a fixed corpus,\n","           this is the size of the corpus. In the truly online setting, this\n","           can be an estimate of the maximum number of documents that\n","           could ever be seen.\n","        alpha: Hyperparameter for prior on weight vectors theta\n","        eta: Hyperparameter for prior on topics beta\n","        tau: A (positive) learning parameter that downweights early iterations\n","        kappa: Learning rate: exponential decay rate---should be between\n","             (0.5, 1.0] to guarantee asymptotic convergence.\n","        Note that if you pass the same set of D documents in every time and\n","        set kappa=0 this class can also be used to do batch VB.\n","    \"\"\"\n","        \n","    def __init__(self, vocab, K, D, alpha, eta, tau, kappa, docs, iterations, parsed = False):\n","        self._vocab = vocab\n","        self._V = len(vocab)\n","        self._K = K\n","        self._D = D\n","        self._alpha = alpha\n","        self._eta = eta\n","        self._tau = tau\n","        self._kappa = kappa\n","        self._lambda = 1* n.random.gamma(100., 1./100., (self._K, self._V))\n","        self._Elogbeta = dirichlet_expectation(self._lambda)\n","        self._expElogbeta = n.exp(self._Elogbeta)\n","        self._docs = docs\n","        self.ct = 0\n","        self._iterations = iterations\n","        self._parsed = parsed\n","        self._trace_lambda = {}\n","        for i in range(self._K):\n","            self._trace_lambda[i] = [self.computeProbabilities()[i]]\n","        self._x = [0]\n","\n","    def updateLocal(self, doc): #word_dn is an indicator variable with dimension V\n","        (words, counts) = doc\n","        newdoc = []\n","        N_d = sum(counts)\n","        phi_d = n.zeros((self._K, N_d))\n","        gamma_d = n.random.gamma(100., 1./100., (self._K))\n","        Elogtheta_d = dirichlet_expectation(gamma_d)\n","        expElogtheta_d = n.exp(Elogtheta_d)\n","        for i, item in enumerate(counts):\n","            for j in range(item):\n","                newdoc.append(words[i])\n","        assert len(newdoc) == N_d, \"error\"\n","\n","        for i in range(self._iterations):\n","            for m, word in enumerate(newdoc):\n","                phi_d[:, m] = n.multiply(expElogtheta_d, self._expElogbeta[:, word]) + 1e-100\n","                phi_d[:, m] = phi_d[:, m]/n.sum(phi_d[:, m])\n","\n","            gamma_new = self._alpha + n.sum(phi_d, axis = 1)\n","            meanchange = n.mean(abs(gamma_d - gamma_new))\n","            if (meanchange < meanchangethresh):\n","                break\n","\n","            gamma_d = gamma_new\n","            Elogtheta_d = dirichlet_expectation(gamma_d)\n","            expElogtheta_d = n.exp(Elogtheta_d)\n","\n","        newdoc = n.asarray(newdoc)\n","        return phi_d, newdoc, gamma_d\n","\n","    def updateGlobal(self, phi_d, doc):\n","\n","        lambda_d = n.zeros((self._K, self._V))\n","\n","        for k in range(self._K):\n","            phi_dk = n.zeros(self._V)\n","            for m, word in enumerate(doc):\n","                phi_dk[word] += phi_d[k][m] \n","            lambda_d[k] = self._eta + self._D * phi_dk\n","        rho = (self.ct + self._tau) **(-self._kappa)\n","        self._lambda = (1-rho) * self._lambda + rho * lambda_d\n","        self._Elogbeta = dirichlet_expectation(self._lambda)\n","        self._expElogbeta = n.exp(self._Elogbeta)\n","\n","        if self.ct % 10 == 9:\n","            for i in range(self._K):\n","                self._trace_lambda[i].append(self.computeProbabilities()[i])\n","            self._x.append(self.ct)\n","\n","    def runSVI(self):\n","        for i in tqdm(range(self._iterations)):\n","            randint = random.randint(0, self._D-1)\n","            if self._parsed == False:\n","                doc = parseDocument(self._docs[randint],self._vocab)\n","            phi_doc, newdoc, gamma_d = self.updateLocal(doc)\n","            self.updateGlobal(phi_doc, newdoc)\n","            self.ct += 1\n","                \n","    def computeProbabilities(self):\n","        prob_topics = n.sum(self._lambda, axis = 1)\n","        prob_topics = prob_topics/n.sum(prob_topics)\n","        return prob_topics\n","\n","    def getTopics(self, docs = None):\n","        prob_topics = self.computeProbabilities()\n","        prob_words = n.sum(self._lambda, axis = 0)\n","\n","        if docs == None:\n","            docs = self._docs\n","        results = n.zeros((len(docs), self._K))\n","        for i, doc in enumerate(docs):\n","            parseddoc = parseDocument(doc, self._vocab)\n","\n","            for j in range(self._K):\n","                aux = [self._lambda[j][word]/prob_words[word] for word in parseddoc[0]]\n","                doc_probability = [n.log(aux[k]) * parseddoc[1][k] for k in range(len(aux))]\n","                results[i][j] = sum(doc_probability) + n.log(prob_topics[j])\n","        finalresults = n.zeros(len(docs))\n","        for k in range(len(docs)):\n","            finalresults[k] = n.argmax(results[k])\n","        return finalresults, prob_topics\n","\n","    def calcPerplexity(self, docs = None):\n","        perplexity = 0.\n","        doclen = 0.\n","        if docs == None:\n","            docs =  self._docs\n","        for doc in docs:\n","            parseddoc = parseDocument(doc, self._vocab)\n","            _, newdoc, gamma_d = self.updateLocal(parseddoc)\n","            approx_mixture = n.dot(gamma_d, self._lambda)\n","            # print(n.shape(approx_mixture))\n","            approx_mixture = approx_mixture / n.sum(approx_mixture)\n","            log_doc_prob = 0.\n","            for word in newdoc:\n","                log_doc_prob += n.log(approx_mixture[word])\n","            perplexity += log_doc_prob\n","            doclen += len(newdoc)\n","            # print(perplexity, doclen)\n","        perplexity = n.exp( - perplexity / doclen)\n","        print(perplexity)\n","        return perplexity\n","\n","    def plotTopics(self, perp):\n","        plottrace(self._x, self._trace_lambda, self._K, self._iterations, perp)\n","\n","def test(k, iterations):\n","\n","    docs = getalldocs(\"alldocs2.txt\")\n","    vocab = getVocab(\"dictionary2.csv\")\n","    \n","    testset = SVILDA(vocab = vocab, K = k, D = len(docs), alpha = 0.2,\n","                     eta = 0.2, tau = 1024, kappa = 0.7, docs = docs,\n","                     iterations= iterations)\n","    testset.runSVI()\n","    finallambda = testset._lambda\n","\n","    heldoutdocs = getalldocs(\"testdocs.txt\")\n","    perplexity = testset.calcPerplexity(docs = heldoutdocs)\n","\n","    with open(\"temp/%i_%i_%f_results.csv\" %(k, iterations, perplexity), \"w+\") as f:\n","        writer = csv.writer(f)\n","        for i in range(k):\n","            bestwords = sorted(range(len(finallambda[i])), key=lambda j:finallambda[i, j])\n","            bestwords.reverse()\n","            writer.writerow([i])\n","            for j, word in enumerate(bestwords):\n","                writer.writerow([word, vocab.keys()[vocab.values().index(word)]])\n","                if j >= 15:\n","                    break\n","    topics, topic_probs = testset.getTopics()\n","    testset.plotTopics(perplexity)\n","\n","    for kk in range(0, len(finallambda)):\n","        lambdak = list(finallambda[kk, :])\n","        lambdak = lambdak / sum(lambdak)\n","        temp = zip(lambdak, range(0, len(lambdak)))\n","        temp = sorted(temp, key = lambda x: x[0], reverse=True)\n","        # print temp\n","        print('topic %d:' % (kk))\n","        # feel free to change the \"53\" here to whatever fits your screen nicely.\n","        for i in range(0, 10):\n","            print('%20s  \\t---\\t  %.4f' % (vocab.keys()[vocab.values().index(temp[i][1])], temp[i][0]))\n","\n","    with open(\"temp/%i_%i_%f_raw.txt\" %(k, iterations, perplexity), \"w+\") as f:\n","        # f.write(finallambda)\n","        for result in topics:\n","            f.write(str(result) + \" \\n\")\n","        f.write(str(topic_probs) + \" \\n\")"]},{"cell_type":"markdown","metadata":{"id":"t5mt0fFfQe3_"},"source":["# Resultados"]},{"cell_type":"markdown","metadata":{"ExecuteTime":{"end_time":"2020-05-26T18:03:47.654905Z","start_time":"2020-05-26T18:03:47.651034Z"},"id":"aJDd0FCVQe3_"},"source":["Una vez definidas todas las funciones necesarias sólo queda ejecutar el análisis de LDA:"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2020-05-26T19:04:50.429919Z","start_time":"2020-05-26T18:55:09.248187Z"},"id":"46G_vbTVQe4A"},"outputs":[],"source":["mode = 'normal'\n","K = 5\n","alpha = 0.2\n","eta = 0.2\n","tau = 1024\n","kappa = 0.7\n","iterations = 100000\n","\n","if mode == \"test\":\n","    test(K, iterations)\n","if mode == \"normal\":\n","    docs = df.abstract.to_list()\n","    D = len(docs)\n","    print('number of docs: {}'.format(D))\n","    lda = SVILDA(vocab = vocab, K = K, D = D, alpha = alpha, \n","                 eta = eta, tau = tau, kappa = kappa, docs = docs, \n","                 iterations = iterations)\n","    lda.runSVI()\n","    lda"]},{"cell_type":"markdown","metadata":{"id":"cBPMG-kXQe4C"},"source":["Se observarán las distribucione de las probabilidades de cada palabra del vocabulario en relación al tema identificado:"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2020-05-26T19:04:54.002460Z","start_time":"2020-05-26T19:04:53.982844Z"},"id":"COTEJXh0Qe4D"},"outputs":[],"source":["lambda_df = pd.DataFrame({'word':list(vocab.keys())})\n","for i in range(K):\n","    lambda_df['Topic {}'.format(i+1)] = lda._lambda[i,:]\n","lambda_df"]},{"cell_type":"markdown","metadata":{"id":"qI0Gi1PWQe4F"},"source":["Para entender cómo el algoritmo de LDA ha calculado la distribución de probabilidades de cada tema podemos observar las 10 palabras con mayor probabilidad por tema:"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2020-05-26T19:04:58.336169Z","start_time":"2020-05-26T19:04:57.118924Z"},"id":"FCd7s7J3Qe4F"},"outputs":[],"source":["finalresults, prob_topics = lda.getTopics()\n","\n","for column,prob in list(zip(lambda_df.columns[1:],prob_topics)):\n","    print('{} probability: {:.2f}%'.format(column,prob*100))\n","    print(lambda_df.nlargest(10, [column])[['word',column]],'\\n')"]},{"cell_type":"markdown","metadata":{"ExecuteTime":{"end_time":"2020-05-26T18:06:52.651043Z","start_time":"2020-05-26T18:06:52.646623Z"},"id":"fzTBn0pxQe4H"},"source":["Finalmente, también podemos observar cómo ha quedado la distribución de temas sobre el conjunto de los documentos descargados:"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2020-05-26T19:04:58.893415Z","start_time":"2020-05-26T19:04:58.834011Z"},"id":"Gi2XptSZQe4H"},"outputs":[],"source":["labels = lambda_df.columns[1:]\n","sizes = prob_topics*100\n","explode = (0.1, 0.1, 0.1,0.1, 0.1) \n","\n","fig1, ax1 = plt.subplots()\n","ax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',\n","        shadow=True, startangle=90)\n","ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n","\n","plt.show()"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"CE_1.2.ipynb","provenance":[]},"hide_input":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":true,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"nbformat":4,"nbformat_minor":0}
